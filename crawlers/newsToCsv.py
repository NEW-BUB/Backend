# pip install feedparser requests beautifulsoup4
import feedparser
import requests
from bs4 import BeautifulSoup, NavigableString
import csv
import traceback
import asyncio
from playwright.async_api import async_playwright
import ast
import re

# ì—°í•©ë‰´ìŠ¤ RSS ì¹´í…Œê³ ë¦¬ ë° URL ì •ë³´
news_source = {
    "ì—°í•©ë‰´ìŠ¤": {
        "rss_url": "https://www.yna.co.kr/rss/",
        "categories": {
            "politics.xml": "ì •ì¹˜",
            "economy.xml": "ê²½ì œ",
            "market.xml": "ê²½ì œ",
            "industry.xml": "ì‚°ì—…",
            "society.xml": "ì‚¬íšŒ",
            "local.xml": "ì§€ì—­",
            "international.xml": "êµ­ì œ",
            "culture.xml": "ë¬¸í™”Â·ë¼ì´í”„",
            "health.xml": "ê±´ê°•",
            "entertainment.xml": "ë¬¸í™”Â·ë¼ì´í”„",
            "sports.xml": "ìŠ¤í¬ì¸ "
        },
        "csv_file": "ì—°í•©ë‰´ìŠ¤ ë°ì´í„°.csv",
    },
    "ê²½í–¥ì‹ ë¬¸": {
        "rss_url": "https://www.khan.co.kr/rss/rssdata/",
        "categories": {
            "politic_news.xml": "ì •ì¹˜",
            "economy_news.xml": "ê²½ì œ",
            "society_news.xml": "ì‚¬íšŒ",
            "local_news.xml": "ì§€ì—­",
            "kh_world.xml": "êµ­ì œ",
            "culture_news.xml": "ë¬¸í™”Â·ë¼ì´í”„",
            "kh_sports.xml": "ìŠ¤í¬ì¸ ",
            "science_news.xml": "ê³¼í•™",
            "life_news.xml": "ë¬¸í™”Â·ë¼ì´í”„"
        },
        "csv_file": "ê²½í–¥ì‹ ë¬¸ ë°ì´í„°.csv",
    },
    "ì¡°ì„ ì¼ë³´": {
        "rss_url": "https://www.chosun.com/arc/outboundfeeds/rss/category/",
        "categories": {
            "politics/?outputType=xml": "ì •ì¹˜",
            "economy/?outputType=xml": "ê²½ì œ",
            "national/?outputType=xml": "ì‚¬íšŒ",
            "international/?outputType=xml": "êµ­ì œ",
            "culture-life/?outputType=xml": "ë¬¸í™”Â·ë¼ì´í”„",
            "sports/?outputType=xml": "ìŠ¤í¬ì¸ ",
            "entertainments/?outputType=xml": "ë¬¸í™”Â·ë¼ì´í”„"
        },
        "csv_file": "ì¡°ì„ ì¼ë³´ ë°ì´í„°.csv",
    },
    "ë™ì•„ì¼ë³´": {
        "rss_url": "https://rss.donga.com/",
        "categories": {
            "politics.xml": "ì •ì¹˜",
            "national.xml": "ì‚¬íšŒ",
            "economy.xml": "ê²½ì œ",
            "international.xml": "êµ­ì œ",
            "science.xml": "ê³¼í•™",
            "culture.xml": "ë¬¸í™”Â·ë¼ì´í”„",
            "sports.xml": "ìŠ¤í¬ì¸ ",
            "health.xml": "ê±´ê°•"
        },
        "csv_file": "ë™ì•„ì¼ë³´ ë°ì´í„°.csv",
    },
    "í•œê²¨ë ˆ": {
        "rss_url": "https://www.hani.co.kr/rss/",
        "categories": {
            "politics": "ì •ì¹˜",
            "economy": "ê²½ì œ",
            "society": "ì‚¬íšŒ",
            "international": "êµ­ì œ",
            "culture": "ë¬¸í™”Â·ë¼ì´í”„",
            "sports": "ìŠ¤í¬ì¸ ",
            "science": "ê³¼í•™"
        },
        "csv_file": "í•œê²¨ë ˆ ë°ì´í„°.csv",
    },
    "jtbc": {
        "rss_url": "https://news-ex.jtbc.co.kr/v1/get/rss/section/",
        "categories": {
            "10": "ì •ì¹˜",
            "20": "ê²½ì œ",
            "30": "ì‚¬íšŒ",
            "40": "êµ­ì œ",
            "50": "ë¬¸í™”Â·ë¼ì´í”„",
            "60": "ë¬¸í™”Â·ë¼ì´í”„",
            "70": "ìŠ¤í¬ì¸ "
        },
        "csv_file": "jtbc ë°ì´í„°.csv",
    }
}
        
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Referer": "https://www.khan.co.kr/",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
}

import signal
import sys

def handle_sigint(sig, frame):
    print("\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C)")
    sys.exit(0)

signal.signal(signal.SIGINT, handle_sigint)

# ë¹„ë™ê¸° Playwright í¬ë¡¤ëŸ¬
async def crawl_news(key, existing_data):
    data = news_source.get(key)
    rss_url = data["rss_url"]
    categories = data["categories"]

    try:
        print(f"\n=={key} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘==\n")
        for url, category in categories.items():
            print(f"\n[{category}] ì¹´í…Œê³ ë¦¬ ì‹œì‘")
            feed = feedparser.parse(rss_url + url)

            if key in ["jtbc"]:
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)

                    try:
                        for entry in feed.entries:
                            link = entry.link

                            if link in existing_data:
                                update_category(existing_data, link, category)
                            else:
                                try:
                                    article_data = await crawl_playwright_news(key, entry, link, browser)
                                    if article_data is None:
                                        continue

                                    article_data["categories"] = [category]
                                    existing_data[link] = article_data

                                    title = article_data["title"]
                                    
                                    print(f"ì €ì¥ ì™„ë£Œ: {title} [{category}]")
                                except Exception as e:
                                    print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {link} ({e})")
                                    print(traceback.format_exc())
                    finally:
                        if browser:
                            for context in browser.contexts:
                                await context.close()
                            await browser.close()
            else:
                for entry in feed.entries:
                    link = entry.link

                    if link in existing_data:
                        update_category(existing_data, link, category)
                    else:
                        try:
                            article_data = crawl_static_news(key, entry, link)
                            if article_data is None:
                                continue
                            article_data["categories"] = [category]
                            existing_data[link] = article_data
                            print(f"ì €ì¥ ì™„ë£Œ: {entry.title} [{category}]")
                        except Exception as e:
                            print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {link} ({e})")
                            print(traceback.format_exc())

    except Exception as e:
        print(f"ì „ì²´ ì‹¤íŒ¨: {e}")


def update_category(existing_data, link, category):
    raw_value = existing_data[link].get("categories", "[]")
    current_categories = ast.literal_eval(raw_value) if isinstance(raw_value, str) else raw_value

    if category not in current_categories:
        current_categories.append(category)
        existing_data[link]["categories"] = current_categories

    title = existing_data[link].get("title", "ì œëª© ì—†ìŒ")
    print(f"ì—…ë°ì´íŠ¸ ì™„ë£Œ: {title} [{category}]")


def crawl_static_news(key, entry, link):
    if key == "ì—°í•©ë‰´ìŠ¤":
        return crawl_yonhap_news(entry, link)
    elif key == "ê²½í–¥ì‹ ë¬¸":
        return crawl_kyunghyang_news(entry, link)
    elif key == "ë™ì•„ì¼ë³´":
        return crawl_donga_news(entry, link)
    elif key == "í•œê²¨ë ˆ":
        return crawl_hani_news(entry, link)
    return {}


async def crawl_playwright_news(key, entry, link, browser):
    if key == "jtbc":
        return await crawl_jtbc_news(entry, link, browser)
    return {}


def crawl_yonhap_news(entry, link):
    response = requests.get(link, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text,"html.parser")
    
    div = soup.find("div", class_="story-news article")
    paragraphs = div.find_all("p")[:-1] if div else []
    text = "\\n".join(p.get_text(strip=True) for p in paragraphs)
    
    if not text or text == "[]" or text == "\n\n" or text == "\n":
        print(link + " ë³¸ë¬¸ ì—†ìŒ")
        return None
    
    if hasattr(entry, "media_content"):
        for media in entry.media_content:
            if media.get("type", "").startswith("image/"):
                img_src = media.get("url")
                break
    else:
        img_src = None
    
    div = soup.find("div", class_="writer-zone01")
    if div:
        a_tags = div.find(class_="tit-name").find_all("a")
        author = [a.get_text(strip=True) for a in a_tags]
    else:
        author = []
    
    article_data = {
        "title": entry.title,
        "link": link,
        "pubDate": entry.published,
        "img_src": img_src,
        "text": text,
        "author": author,
    }
    
    return article_data

def crawl_kyunghyang_news(entry, link):
    response = requests.get(link, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    
    div = soup.find("div", class_="art_body")
    paragraphs = div.find_all("p") if div else []
    text = "\\n".join(p.get_text(strip=True) for p in paragraphs)
    
    if not text or text == "[]" or text == "\n\n" or text == "\n":
        print(link + " ë³¸ë¬¸ ì—†ìŒ")
        return None
    
    div = soup.find("div", class_="art_photo")
    img_src = div.find("img")["src"] if div and div.find("img") else None
    
    ul = soup.find("ul", class_="bottom")
    if ul:
        a_tags = ul.find("li", class_="editor").find_all("a")
        author = [a.get_text(strip=True) for a in a_tags]
    else:
        author = []
    
    article_data = {
        "title": entry.title,
        "link": link,
        "pubDate": entry.updated,
        "img_src": img_src,
        "text": text,
        "author": author
    }
    
    return article_data
                    
def crawl_donga_news(entry, link):
    response = requests.get(link, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    
    if hasattr(entry, "media_content"):
        for media in entry.media_content:
            if media.get("type", "").startswith("image/"):
                img_src = media.get("url")
                break
    else:
        img_src = None

    article_div = soup.find('section', class_='news_view')
    if not article_div:
        print(link + " ë³¸ë¬¸ ì—†ìŒ")
        return None

    texts = []
    for element in article_div.descendants:
        if isinstance(element, NavigableString):
            parent = element.parent
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ë¥¼ ì œì™¸
            if parent.name not in ['script', 'style', 'p', 'br', 'div', 'body', 'html']:
                text = element.strip()
                if text:
                    texts.append(text)
    text = '\\n'.join(texts)
    text = text.replace("BYLINE", "").replace("//BYLINE", "").replace("//", "").strip()
    
    if not text or text == "[]" or text == "\n\n" or text == "\n":
        print(link + " ë³¸ë¬¸ ì—†ìŒ")
        return None
    
    byline_div = soup.find("div", class_="byline")
    if byline_div:
        byline_text = byline_div.get_text(separator="\n")
        author = re.findall(r"([\uac00-\ud7a3]{2,4}) ê¸°ì", byline_text)
    else:
        author = []
        
    article_data = {
        "title": entry.title,
        "link": link,
        "pubDate": entry.updated,
        "img_src": img_src,
        "text": text,
        "author": author,
    }
    return article_data

def crawl_hani_news(entry, link):
    response = requests.get(link, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    
    div = soup.find("div", class_="article-text")
    paragraphs = div.find_all("p")[:] if div else []
    text = "\\n".join(p.get_text(strip=True) for p in paragraphs)

    if not text or text == "[]" or text == "\n\n" or text == "\n":
        print(link + " ë³¸ë¬¸ ì—†ìŒ")
        return None
    
    video_div = soup.find("div", class_="video-wrap")  # ë¹„ë””ì˜¤ ìˆëŠ”ì§€ í™•ì¸

    if video_div is None:  # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ ì°¾ê¸°
        img_div = soup.find("div", class_="ArticleDetailContent_imageWrap__o8GzH")
        img_src = img_div.find("img")["src"] if img_div and img_div.find("img") else None
    else:
        img_src = None  # ë¹„ë””ì˜¤ ìˆì„ ë• ì´ë¯¸ì§€ ë¬´ì‹œ
    
    div = soup.find("div", class_="ArticleDetailView_articleDetail__IT2fh")
    time = div.find("li", class_="ArticleDetailView_dateListItem__mRc3d").find("span").get_text()
    
    div = soup.find("div", class_="ArticleDetailView_reporterList__waOKp")
    if div:
        a_tags = div.find_all("a")
        author = [a.get_text(strip=True) for a in a_tags]
    else:
        author = []
    
    article_data = {"title": entry.title,
        "link": link,
        "pubDate": time,
        "img_src": img_src,
        "text": text,
        "author": author,
    }
    return article_data

async def crawl_jtbc_news(entry, link, browser):
    response = requests.get(link, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    
    meta_tag = soup.find("meta", property="og:image")

    # content ì†ì„± ê°’ ì¶”ì¶œ
    if meta_tag:
        img_src = meta_tag.get("content", "")
    else:
        img_src = None
    
    meta_tag = soup.find("meta", attrs={"name": "Author"})

    if meta_tag:
        content = meta_tag.get("content", "")
        author = [name.strip() for name in content.split(",")]
    else:
        author = []

    page = await browser.new_page()
    try:
        await page.goto(link, timeout=40000)
        await page.wait_for_selector("div.my-9iwogb", timeout=15000)
        html = await page.content()

        soup = BeautifulSoup(html, "html.parser")
        
        content_div = soup.select_one("div.my-9iwogb")

        for br in content_div.find_all("br"):
            br.replace_with("\\n")

        texts = [el.get_text(strip=True) for el in content_div.find_all(["span", "strong"])]
        text = "\\n".join(dict.fromkeys(texts))

        if not text or text == "[]" or text == "\n\n" or text == "\n":
            print(link + " ë³¸ë¬¸ ì—†ìŒ")
            return None
        
        article_data = {
            "title": entry.title,
            "link": link,
            "pubDate": entry.updated,
            "img_src": img_src,
            "text": text,
            "author": author,
        }
        return article_data
    finally:
        await page.close()
