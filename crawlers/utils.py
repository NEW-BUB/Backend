def get_keyword(stopwords, pref):
    # pip install krwordrank
    from krwordrank.word import summarize_with_keywords
    import re
    
    if not pref : return

    pref = [pref]  # ë‹¨ì¼ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    keywords = summarize_with_keywords(
        texts=pref,
        min_count=2,
        max_length=10,
        beta=0.85,
        max_iter=10
    )

    def clean_and_validate(word):
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        cleaned = re.sub(r'[,\.\(\)\[\]]', '', word)

        # ìˆ«ì or ë¶ˆìš©ì–´ or ì–´ë¯¸ë¡œ ëë‚˜ë©´ ì œì™¸
        if cleaned in stopwords:
            return None
        if re.fullmatch(r'\d+(\.\d+)?', cleaned):  # ì •ìˆ˜ ë˜ëŠ” ì†Œìˆ˜
            return None
        if re.search(r'(ë‹¤|ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ëœë‹¤|ì´ë‹¤|í•œë‹¤|ëë‹¤)$', cleaned):
            return None
        if not cleaned.strip():  # ë¹ˆ ë¬¸ìì—´ ì œê±°
            return None
        return cleaned

    cleaned_keywords = {}
    for word, score in sorted(keywords.items(), key=lambda x: -x[1])[:15]:
        cleaned = clean_and_validate(word)
        if cleaned:
            cleaned_keywords[cleaned] = score

    return list(cleaned_keywords.keys())


def get_news(url):
    # pip install newspaper3k lxml_html_clean
    from newspaper import Article 
    import re
    
    def clean_text(text):
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue  # ë¹ˆ ì¤„ ì œê±°

            # ì¤„ ì „ì²´ ì œê±° ì¡°ê±´ ì¶”ê°€: VOD ì•ˆë‚´, ë¸Œë¼ìš°ì € ì—…ê·¸ë ˆì´ë“œ ë“± ì•ˆë‚´ ë¬¸êµ¬ ì œê±°
            if re.search(r'(VOD ì‹œì²­ ì•ˆë‚´|ì–´ë„ë¹„ í”Œë˜ì‹œ í”Œë ˆì´ì–´|ë¸Œë¼ìš°ì € ì—…ê·¸ë ˆì´ë“œ|ì„œë¹„ìŠ¤ê°€ ì›í• í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)', line):
                continue

            # ì¤„ ì „ì²´ ì œê±° ì¡°ê±´: 'ì „ê²½.', 'ëª¨ìŠµ.', 'â“’' í¬í•¨
            if re.search(r'(â“’|Â©|[ì „ê²½|ëª¨ìŠµ]\.)', line):
                continue  # ì¤„ ìì²´ ì œê±°

            # ë¶€ë¶„ ì œê±° ì¡°ê±´: ê¸°ì ì´ë¦„, ì´ë©”ì¼, ì¢‹ì•„ìš” ë“±
            line = re.sub(r'[ê°€-í£]+ ?ê¸°ì', '', line)  # ê¸°ì ì´ë¦„
            line = re.sub(r'[\w\.-]+@[\w\.-]+', '', line)  # ì´ë©”ì¼
            line = re.sub(r'(ì‘ì›ìˆ˜|ì¢‹ì•„ìš”|ì¶”ì²œìˆ˜|ì´ë¯¸ì§€ í™•ëŒ€|ë‹«ê¸°)[^\n]*', '', line)  # ì¢‹ì•„ìš”, ì‘ì›ìˆ˜ ë“±

            cleaned_lines.append(line.strip())

        return '\n'.join(cleaned_lines).strip()

    
    # ë¶ˆìš©ì–´ ë° ì œì™¸ ì¡°ê±´
    stopwords = {
        'ì‹¶ì€', 'ì‘ê°€ì˜', 'ì•„ë‹ˆ', 'ì´ì•¼ê¸°', 'ê·¸ë¦¬ê³ ', 'ìˆëŠ”', 'ìˆì„', 'ìœ„í•œ', 'ë˜ì–´', 'ëŒ€ìƒ',
        'ìœ„í•´', 'ê°™ì€', 'ë˜ëŠ”', 'ë˜', 'ì´ì™€', 'ì´ë¥¼', 'ì´ë²ˆ', 'ë“±ì„', 'ë˜í•œ', 'ë”ìš±', 'ìˆë„ë¡',
        'í¬í•¨í•œ', 'ìµœëŒ€', 'ë§¤ì¼'
    }

    print(f"\nğŸ“° ê¸°ì‚¬ URL: {url}")

    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()

        title = article.title
        date = article.publish_date
        image = article.top_image
        authors = article.authors
        text = clean_text(article.text)

        print("ì œëª©:", title)
        print("ë°œí–‰ì¼:", date)
        print("ëŒ€í‘œ ì´ë¯¸ì§€ URL:", image)
        if authors: print("ì‘ì„±ì:", authors)
        print("ë³¸ë¬¸ ì¼ë¶€:", text[:100], "...")
        # print("ì›ë˜ ë³¸ë¬¸:", article.text)
        print("ë³€ê²½ëœ ë³¸ë¬¸:", text)
        print("í‚¤ì›Œë“œ:", get_keyword(stopwords, text))
    except Exception as e:
        print("ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨:", e)