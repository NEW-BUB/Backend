def get_keyword(texts, stopwords=None):
    from krwordrank.word import summarize_with_keywords
    import pandas as pd
    import re

    # ğŸ”¸ ê¸°ë³¸ ë¶ˆìš©ì–´ ì„¤ì •
    stopwords += [
        'ìˆë‹¤', 'í•œë‹¤', 'ìœ„í•´', 'ëŒ€í•œ', 'í†µí•´', 'ë°', 'ë“±', 'ì œ', 'ì˜', 'ìˆëŠ”', 
        'í•˜ë©°', 'í•˜ê³ ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ê·¸ë¦¬ê³ ', 'ê·¸', 'ìœ„í•œ', 'ë„ë‚´', 'ì´ì „', 
        'ì§€ë‚œ', 'ë‚´ë…„', 'ìˆìŒì„', 'ê·¸ëŠ”', 'í•˜ëŠ”', 'ê·¸ì˜', 'ì´í›„', 'í–ˆë‹¤', 'ê·¸ë…€', 
        'ì €', 'ì´ëŸ°', 'ìµœê·¼', 'ë•Œë¬¸', 'ê´€ë ¨', 'ë‹¤ë¥¸', 'í•œ', 'ì´ë‹¤', 'í•˜ë‹¤', 'ì„', 
        'ë¥¼', 'ì´', 'ê°€', 'ë“¤', 'ì™€', 'ê³¼', 'íŠ¹íˆ', 'ì¶œì‹œ', 'êµ­ë‚´', 'ìµœì‹ ', 'í•¨ê»˜', 
        'ê°™ì´', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ê²Œë‹¤ê°€', 'ë˜ëŠ”', 'ë¿ë§Œ ì•„ë‹ˆë¼', 
        'ì´ë¼ë©´', 'ì´ë¼ì„œ', 'ì˜€ìœ¼ë©°', 'í–ˆë˜', 'ë•Œë¬¸ì—', 'ë“±ì„', 'ì˜í•´', 'ìœ¼ë¡œì¨', 
        'ë§¤ìš°', 'êµ‰ì¥íˆ', 'ì•„ì£¼', 'ì˜', 'ì¡°ê¸ˆ', 'ì¢€', 'ê°™ì€', 'ì •ë§', 'ë§ì€', 'ëª¨ë“ ', 
        'ì„œë¡œ', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ì§€ê¸ˆ', 'í˜„ì¬', 'ë‹¤ê°€ì˜¤ëŠ”', 'ì•ìœ¼ë¡œ', 'ì§€ë‚œë²ˆ', 
        'ì˜¬í•´', 'ìš°ë¦¬', 'ë„ˆí¬', 'ë„ˆ', 'ì €í¬', 'ìì‹ ', 'ëˆ„êµ¬', 'ê°ì', 'ëª¨ë“  ì‚¬ëŒ', 
        'ì‚¬ëŒë“¤', 'ê·¸ê²ƒ', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì™œ', 'ë‹¤ì‹œ', 'ê±°ê¸°', 'ì €ê¸°', 'ì—¬ê¸°', 'ê±°ì˜', 
        'ëŒ€ë¶€ë¶„', 'ë‹¹ì‹œ', 'ê·¸ë‚ ', 'ë‹¤ìŒ', 'ê·¸ë•Œ', 'ì´ë²ˆ', 'ì–¸ì œë‚˜', 'í•­ìƒ', 'ìì£¼', 
        'ê°€ë”', 'ì¢…ì¢…', 'í•œë²ˆ', 'ì •ë„', 'ì•½ê°„', 'ëŒ€ëµ', 'ì™„ì „íˆ', 'ì „í˜€', 'ë”ë¶ˆì–´', 
        'ì‹¬ì§€ì–´', 'ë”êµ¬ë‚˜', 'í™•ì‹¤íˆ', 'ë¶„ëª…íˆ', 'ìˆë‹¤ê³ ', 'ê°€ìš´ë°', 'ì˜¤í›„', 'ì´ë¼ë©°',
        'ì´ë¼', 'ê²½ìš°', 'ê²°ê³¼', 'ì´ë‹¬', 'ìˆì–´', 'ëŒ€í•´', 'ê¸°ì¡´', 'í–¥í›„', 'ë¹„ë¡¯'
    ]
        

    # âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    texts = texts.lower()  # ëª¨ë“  ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
    texts = re.sub(r'[^ê°€-í£a-z\s]', ' ', texts)  # í•œê¸€, ì˜ë¬¸, ê³µë°±ë§Œ ë‚¨ê¹€

    # âœ… ë¬¸ì¥ ë¶„ë¦¬ ë° ì •ë¦¬
    sentences = re.split(r'[.\n]', texts)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]

    # âœ… í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = summarize_with_keywords(
        sentences,
        min_count=2,
        max_length=10,
        beta=0.85,
        max_iter=20
    )
    
    filtered_keywords = {
        k: v for k, v in keywords.items()
        if not re.match(r'.*(ë‹¤|ì´ë‹¤|ìˆë‹¤|í•œë‹¤|í–ˆë‹¤|í•˜ë‹¤|ë³´ë‹¤|í•œ|ë¥¼|ë |ì„|ì´|ê°€|ì—|ì˜|ì€|ëŠ”|ë¡œ|ê¹Œì§€|ì—|ê²Œ|ì |ê³ |ë©°|ë©´|ì„œ|ë¶€í„°)$', k)  # ì¡°ì‚¬/ì–´ë¯¸ë¡œ ëë‚˜ëŠ” í‚¤ì›Œë“œ ì œê±°
    }

    # ğŸ”¸ DataFrame ë³€í™˜
    df = pd.DataFrame(
        sorted(filtered_keywords.items(), key=lambda x: -x[1]), 
        columns=['Keyword', 'Score']
    )

    # ë¶ˆìš©ì–´ ì œê±°: 'Keyword' ì»¬ëŸ¼ì—ì„œ ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•œ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    df['Keyword'] = df['Keyword'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))

    # ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•œ í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ê¸° (ë¶ˆìš©ì–´ê°€ í¬í•¨ëœ í‚¤ì›Œë“œëŠ” ì œê±°ë¨)
    df = df[df['Keyword'].str.strip() != '']
    
    return df.head(20)



def get_news(url):
    # pip install newspaper3k lxml_html_clean
    from newspaper import Article 
    import re
    
    def clean_text(text):
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue  # ë¹ˆ ì¤„ ì œê±°

            # ì¤„ ì „ì²´ ì œê±° ì¡°ê±´ ì¶”ê°€: VOD ì•ˆë‚´, ë¸Œë¼ìš°ì € ì—…ê·¸ë ˆì´ë“œ ë“± ì•ˆë‚´ ë¬¸êµ¬ ì œê±°
            if re.search(r'(VOD ì‹œì²­ ì•ˆë‚´|ì–´ë„ë¹„ í”Œë˜ì‹œ í”Œë ˆì´ì–´|ë¸Œë¼ìš°ì € ì—…ê·¸ë ˆì´ë“œ|ì„œë¹„ìŠ¤ê°€ ì›í• í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)', line):
                continue

            # ì¤„ ì „ì²´ ì œê±° ì¡°ê±´: 'ì „ê²½.', 'ëª¨ìŠµ.', 'â“’' í¬í•¨
            if re.search(r'(â“’|Â©|[ì „ê²½|ëª¨ìŠµ]\.)', line):
                continue  # ì¤„ ìì²´ ì œê±°

            # ë¶€ë¶„ ì œê±° ì¡°ê±´: ê¸°ì ì´ë¦„, ì´ë©”ì¼, ì¢‹ì•„ìš” ë“±
            line = re.sub(r'[ê°€-í£]+ ?ê¸°ì', '', line)  # ê¸°ì ì´ë¦„
            line = re.sub(r'[\w\.-]+@[\w\.-]+', '', line)  # ì´ë©”ì¼
            line = re.sub(r'(ì‘ì›ìˆ˜|ì¢‹ì•„ìš”|ì¶”ì²œìˆ˜|ì´ë¯¸ì§€ í™•ëŒ€|ë‹«ê¸°)[^\n]*', '', line)  # ì¢‹ì•„ìš”, ì‘ì›ìˆ˜ ë“±

            cleaned_lines.append(line.strip())

        return '\n'.join(cleaned_lines).strip()

    
    # ë¶ˆìš©ì–´ ë° ì œì™¸ ì¡°ê±´
    stopwords = [
        "ì‘ê°€ì˜", "ì´ì•¼ê¸°", "í¬í•¨í•œ", "ìµœëŒ€", "ë§¤ì¼", "ì¶œì‹œ", "êµ­ë‚´", "ì •ë§ë¡œ", 
        "ì‚¬ì‹¤", "ë³´ë„ì— ë”°ë¥´ë©´", "ì¶œì²˜", "ê¸°ì", "ê²°êµ­", "ë”°ë¼ì„œ","ì¦‰", "ê·¸ë˜ì„œ", 
        "ê²°ê³¼ì ìœ¼ë¡œ", "ê²€ìƒ‰", "ë¬¸ì œ", "ì¦ê¸¸", "ë†€ëŸ¬", "ë‚˜ë“¤", "ì—´ë¦°", "ë„˜ì–´"
    ]

    print(f"\nğŸ“° ê¸°ì‚¬ URL: {url}")

    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()

        title = article.title
        date = article.publish_date
        image = article.top_image
        authors = article.authors
        text = clean_text(article.text)

        print("ì œëª©:", title)
        print("ë°œí–‰ì¼:", date)
        print("ëŒ€í‘œ ì´ë¯¸ì§€ URL:", image)
        if authors: print("ì‘ì„±ì:", authors)
        print("ë³¸ë¬¸ ì¼ë¶€:", text[:100], "...")
        # print("ì›ë˜ ë³¸ë¬¸:", article.text)
        print("ë³€ê²½ëœ ë³¸ë¬¸:", text)
        print("í‚¤ì›Œë“œ:", get_keyword(text, stopwords))
    except Exception as e:
        print("ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨:", e)